{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '20news-bydate 2/matlab/'\n",
      "/Users/phesami/Documents/DSE/phesami/DSE210/Week2/20news-bydate 2/matlab\n"
     ]
    }
   ],
   "source": [
    "%cd 20news-bydate\\ 2\\matlab/\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def navive_bayes_train (data_DF, label_DF, map_DF, num_words, frequency_method):\n",
    "    ####joing the label and the data####\n",
    "    label_DF.index+=1\n",
    "    data_label_DF=data_DF.join(label_DF, on='docIdx')\n",
    "\n",
    "    ####creating a dummy DF which includes all words ID with count 1####\n",
    "    dummy_missing_words=pd.DataFrame(np.zeros(num_words), columns=['dummy_count']).reset_index().rename(columns={'index':'wordIdx'})+1\n",
    "\n",
    "    ####calculating the probability of each class. ie., P_j####\n",
    "    total_docs=data_label_DF.groupby('docLabel')['docIdx'].count().sum()\n",
    "    Pj=data_label_DF.groupby('docLabel')['docIdx'].count()/total_docs\n",
    "    log_Pj=np.log(Pj)\n",
    "\n",
    "    ####calculating the log of the probability of each word in each class. i.e., log(P_ji)\n",
    "    word_count_Pj_DF=pd.DataFrame()\n",
    "    for i in range(1,map_DF.shape[0]+1):\n",
    "\n",
    "        ####joining the main dataframe with the dummy dataframe to include the missing words in each documents####\n",
    "        word_count_DF=data_label_DF.ix[data_label_DF['docLabel']==i].merge(dummy_missing_words, on='wordIdx', how='right').sort('wordIdx')\n",
    "        word_count_DF['count'].fillna(value=0, inplace=True)\n",
    "        ####laplace smoothing is done at this step automatically as every word count is incremented by 1####\n",
    "        ####adding the log(1+f) instead of f as an option for the last part of t\n",
    "        if (frequency_method=='log'):\n",
    "            word_count_DF['word_count_'+str(i)]=np.log(1+word_count_DF['count']+word_count_DF['dummy_count'])\n",
    "        else:\n",
    "            word_count_DF['word_count_'+str(i)]=word_count_DF['count']+word_count_DF['dummy_count']\n",
    "\n",
    "        word_count_DF.drop(['count', 'dummy_count','docIdx', 'docLabel'], axis=1, inplace=True)\n",
    "        word_count_Pj_temp_DF= word_count_DF.groupby('wordIdx').sum()\n",
    "        word_count_Pj_temp_DF['log(P_'+str(i)+'i)']=np.log((word_count_Pj_temp_DF['word_count_'+str(i)])/(word_count_Pj_temp_DF['word_count_'+str(i)].sum()))\n",
    "        word_count_Pj_DF=pd.concat([word_count_Pj_DF,word_count_Pj_temp_DF['log(P_'+str(i)+'i)']], axis=1)\n",
    "    ####returns P_j and log(P_ji)####\n",
    "    return word_count_Pj_DF, log_Pj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive_bayes_multinomial_classifier(data_DF, label_DF, word_count_Pj_DF, log_Pj):\n",
    "    start = time.time()\n",
    "    dummy_missing_words_test=pd.DataFrame(np.zeros(num_words), columns=['dummy_count']).reset_index().rename(columns={'index':'wordIdx'})\n",
    "    dummy_missing_words_test['wordIdx']+=1\n",
    "    dummy_missing_words_test.head()\n",
    "    results={}\n",
    "    for doc in data_DF['docIdx'].unique():\n",
    "        data_train_count_DF=data_DF.ix[data_DF['docIdx']==doc].merge(dummy_missing_words_test, on='wordIdx', how='right').sort('docIdx')\n",
    "        data_train_count_DF['count'].fillna(value=0, inplace=True)\n",
    "        data_train_count_DF['docIdx'].fillna(value=doc, inplace=True)\n",
    "        data_train_count_DF['word_count']=data_train_count_DF['count']+data_train_count_DF['dummy_count']\n",
    "        word_count_sorted= data_train_count_DF.sort(columns='wordIdx')['word_count']\n",
    "        word_count_rep=pd.concat([word_count_sorted]*20, axis=1)\n",
    "        x_log=pd.DataFrame(word_count_rep.values*word_count_Pj_DF.values, columns=range(1,21),index=word_count_Pj_DF.index).sum()\n",
    "        results[doc]= pd.DataFrame(x_log+log_Pj).idxmax()[0]\n",
    "\n",
    "    error=np.mean(results.values()!=label_DF)*100\n",
    "    end = time.time()\n",
    "    print \"error rate is:\\n\",error[0],'%'\n",
    "    print \"time it took on the data set:\\n\",int(end-start)/60, \"minutes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log(P_1i)</th>\n",
       "      <th>log(P_2i)</th>\n",
       "      <th>log(P_3i)</th>\n",
       "      <th>log(P_4i)</th>\n",
       "      <th>log(P_5i)</th>\n",
       "      <th>log(P_6i)</th>\n",
       "      <th>log(P_7i)</th>\n",
       "      <th>log(P_8i)</th>\n",
       "      <th>log(P_9i)</th>\n",
       "      <th>log(P_10i)</th>\n",
       "      <th>log(P_11i)</th>\n",
       "      <th>log(P_12i)</th>\n",
       "      <th>log(P_13i)</th>\n",
       "      <th>log(P_14i)</th>\n",
       "      <th>log(P_15i)</th>\n",
       "      <th>log(P_16i)</th>\n",
       "      <th>log(P_17i)</th>\n",
       "      <th>log(P_18i)</th>\n",
       "      <th>log(P_19i)</th>\n",
       "      <th>log(P_20i)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wordIdx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.626822</td>\n",
       "      <td>-8.037989</td>\n",
       "      <td>-9.107572</td>\n",
       "      <td>-9.615915</td>\n",
       "      <td>-9.867290</td>\n",
       "      <td>-8.404160</td>\n",
       "      <td>-11.960255</td>\n",
       "      <td>-9.595849</td>\n",
       "      <td>-9.415573</td>\n",
       "      <td>-11.637980</td>\n",
       "      <td>-11.812874</td>\n",
       "      <td>-8.334283</td>\n",
       "      <td>-10.900279</td>\n",
       "      <td>-9.579829</td>\n",
       "      <td>-8.348430</td>\n",
       "      <td>-12.770284</td>\n",
       "      <td>-9.627332</td>\n",
       "      <td>-10.048035</td>\n",
       "      <td>-12.677129</td>\n",
       "      <td>-12.345487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.902074</td>\n",
       "      <td>-7.757374</td>\n",
       "      <td>-7.462416</td>\n",
       "      <td>-8.323147</td>\n",
       "      <td>-8.126824</td>\n",
       "      <td>-6.905562</td>\n",
       "      <td>-8.110108</td>\n",
       "      <td>-7.804090</td>\n",
       "      <td>-7.651985</td>\n",
       "      <td>-8.236782</td>\n",
       "      <td>-7.814674</td>\n",
       "      <td>-7.774667</td>\n",
       "      <td>-8.226131</td>\n",
       "      <td>-8.370869</td>\n",
       "      <td>-7.619178</td>\n",
       "      <td>-7.476979</td>\n",
       "      <td>-8.611412</td>\n",
       "      <td>-7.517872</td>\n",
       "      <td>-8.443022</td>\n",
       "      <td>-8.014753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.639458</td>\n",
       "      <td>-12.300669</td>\n",
       "      <td>-12.198615</td>\n",
       "      <td>-12.254972</td>\n",
       "      <td>-12.169875</td>\n",
       "      <td>-12.515034</td>\n",
       "      <td>-11.960255</td>\n",
       "      <td>-12.368438</td>\n",
       "      <td>-12.305945</td>\n",
       "      <td>-12.331127</td>\n",
       "      <td>-12.506022</td>\n",
       "      <td>-12.765100</td>\n",
       "      <td>-12.286574</td>\n",
       "      <td>-12.575562</td>\n",
       "      <td>-12.567938</td>\n",
       "      <td>-9.402988</td>\n",
       "      <td>-12.671855</td>\n",
       "      <td>-12.938407</td>\n",
       "      <td>-12.677129</td>\n",
       "      <td>-9.512273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.878137</td>\n",
       "      <td>-9.081793</td>\n",
       "      <td>-8.902778</td>\n",
       "      <td>-12.254972</td>\n",
       "      <td>-11.476728</td>\n",
       "      <td>-7.919914</td>\n",
       "      <td>-10.573961</td>\n",
       "      <td>-12.368438</td>\n",
       "      <td>-10.514186</td>\n",
       "      <td>-10.944833</td>\n",
       "      <td>-12.506022</td>\n",
       "      <td>-9.874728</td>\n",
       "      <td>-11.187961</td>\n",
       "      <td>-9.484519</td>\n",
       "      <td>-8.904376</td>\n",
       "      <td>-10.285377</td>\n",
       "      <td>-10.186948</td>\n",
       "      <td>-10.048035</td>\n",
       "      <td>-9.093610</td>\n",
       "      <td>-10.959192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-7.729702</td>\n",
       "      <td>-9.081793</td>\n",
       "      <td>-8.643267</td>\n",
       "      <td>-9.310533</td>\n",
       "      <td>-11.476728</td>\n",
       "      <td>-9.296158</td>\n",
       "      <td>-10.861643</td>\n",
       "      <td>-9.323916</td>\n",
       "      <td>-10.226504</td>\n",
       "      <td>-11.637980</td>\n",
       "      <td>-11.812874</td>\n",
       "      <td>-8.383073</td>\n",
       "      <td>-10.089349</td>\n",
       "      <td>-9.020214</td>\n",
       "      <td>-9.476895</td>\n",
       "      <td>-9.474447</td>\n",
       "      <td>-9.627332</td>\n",
       "      <td>-11.552113</td>\n",
       "      <td>-9.309833</td>\n",
       "      <td>-8.848979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         log(P_1i)  log(P_2i)  log(P_3i)  log(P_4i)  log(P_5i)  log(P_6i)  \\\n",
       "wordIdx                                                                     \n",
       "1        -9.626822  -8.037989  -9.107572  -9.615915  -9.867290  -8.404160   \n",
       "2        -7.902074  -7.757374  -7.462416  -8.323147  -8.126824  -6.905562   \n",
       "3        -6.639458 -12.300669 -12.198615 -12.254972 -12.169875 -12.515034   \n",
       "4        -9.878137  -9.081793  -8.902778 -12.254972 -11.476728  -7.919914   \n",
       "5        -7.729702  -9.081793  -8.643267  -9.310533 -11.476728  -9.296158   \n",
       "\n",
       "         log(P_7i)  log(P_8i)  log(P_9i)  log(P_10i)  log(P_11i)  log(P_12i)  \\\n",
       "wordIdx                                                                        \n",
       "1       -11.960255  -9.595849  -9.415573  -11.637980  -11.812874   -8.334283   \n",
       "2        -8.110108  -7.804090  -7.651985   -8.236782   -7.814674   -7.774667   \n",
       "3       -11.960255 -12.368438 -12.305945  -12.331127  -12.506022  -12.765100   \n",
       "4       -10.573961 -12.368438 -10.514186  -10.944833  -12.506022   -9.874728   \n",
       "5       -10.861643  -9.323916 -10.226504  -11.637980  -11.812874   -8.383073   \n",
       "\n",
       "         log(P_13i)  log(P_14i)  log(P_15i)  log(P_16i)  log(P_17i)  \\\n",
       "wordIdx                                                               \n",
       "1        -10.900279   -9.579829   -8.348430  -12.770284   -9.627332   \n",
       "2         -8.226131   -8.370869   -7.619178   -7.476979   -8.611412   \n",
       "3        -12.286574  -12.575562  -12.567938   -9.402988  -12.671855   \n",
       "4        -11.187961   -9.484519   -8.904376  -10.285377  -10.186948   \n",
       "5        -10.089349   -9.020214   -9.476895   -9.474447   -9.627332   \n",
       "\n",
       "         log(P_18i)  log(P_19i)  log(P_20i)  \n",
       "wordIdx                                      \n",
       "1        -10.048035  -12.677129  -12.345487  \n",
       "2         -7.517872   -8.443022   -8.014753  \n",
       "3        -12.938407  -12.677129   -9.512273  \n",
       "4        -10.048035   -9.093610  -10.959192  \n",
       "5        -11.552113   -9.309833   -8.848979  "
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####Data importing and prepration####\n",
    "data_DF=pd.read_csv(\"train.data\", header=None, names=['docIdx','wordIdx','count'], sep=r\"\\s+\")\n",
    "map_DF=pd.read_csv(\"train.map\", header=None, names=['category','categoryIdx'], sep=r\"\\s+\")\n",
    "label_DF=pd.read_csv(\"train.label\", header=None, names=['docLabel'], sep=r\"\\s+\")\n",
    "num_words=pd.read_csv('vocabulary.txt').shape[0]\n",
    "\n",
    "####training####\n",
    "word_count_Pj_DF, log_Pj= navive_bayes_train (data_DF, label_DF, map_DF, num_words, 'normal')\n",
    "word_count_Pj_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####reloading data into new dataframe names to avoid confusion####\n",
    "data_train_DF=pd.read_csv(\"train.data\", header=None, names=['docIdx','wordIdx','count'], sep=r\"\\s+\")\n",
    "data_test_DF=pd.read_csv(\"test.data\", header=None, names=['docIdx','wordIdx','count'], sep=r\"\\s+\")\n",
    "label_train_DF=pd.read_csv(\"train.label\", header=None, names=['docLabel'], sep=r\"\\s+\")\n",
    "label_test_DF=pd.read_csv(\"test.label\", header=None, names=['docLabel'], sep=r\"\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate is:\n",
      "4.60555506256 %\n",
      "time it took on the data set:\n",
      "16 minutes\n"
     ]
    }
   ],
   "source": [
    "####classifying the train data####\n",
    "naive_bayes_multinomial_classifier(data_train_DF, label_train_DF, word_count_Pj_DF, log_Pj) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate is:\n",
      "20.253164557 %\n",
      "time it took on the data set:\n",
      "9 minutes\n"
     ]
    }
   ],
   "source": [
    "####classifying the test data####\n",
    "naive_bayes_multinomial_classifier(data_test_DF, label_test_DF, word_count_Pj_DF, log_Pj) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of the Naive Bayes Classifier ###\n",
    "**1. Error rate on train set: 4.6%**\n",
    "\n",
    "**2. Error rate on test set: 20.25%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####part f-Optional###\n",
    "####splitting the train data into 80% train set and 20% validation set####\n",
    "####as our data and label are not the same size, we split label only and join the splitted label on data to derive splitted data####\n",
    "from sklearn.cross_validation import train_test_split\n",
    "label_train_split, label_train_val = train_test_split(label_train_DF, test_size=0.2)\n",
    "label_train_split=label_train_split.sort()\n",
    "label_train_val=label_train_val.sort()\n",
    "data_train_DF_split = data_train_DF.join(label_train_split, on='docIdx', how='right').drop(['docLabel'], axis=1).reset_index(drop=True)\n",
    "data_train_DF_val = data_train_DF.join(label_train_val, on='docIdx', how='right').drop(['docLabel'], axis=1).reset_index(drop=True)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate is:\n",
      "8.07542983916 %\n",
      "time it took on the data set:\n",
      "13 minutes\n"
     ]
    }
   ],
   "source": [
    "####training on the 80% of the date with replacing f with log(1+f) as word frequency####\n",
    "word_count_Pj_DF_split, log_Pj_split = navive_bayes_train (data_train_DF_split, label_train_split, map_DF, num_words, 'log')\n",
    "####measuring error with the 20% validation set with replacing f with log(1+f) as word frequency####\n",
    "naive_bayes_multinomial_classifier(data_train_DF_split, label_train_split, word_count_Pj_DF_split, log_Pj_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate is:\n",
      "20.6662225183 %\n",
      "time it took on the data set:\n",
      "10 minutes\n"
     ]
    }
   ],
   "source": [
    "####Naive Bayes results with log(1+f) instead of f###\n",
    "word_count_Pj_DF, log_Pj= navive_bayes_train (data_DF, label_DF, map_DF, num_words, 'log')\n",
    "naive_bayes_multinomial_classifier(data_test_DF, label_test_DF, word_count_Pj_DF, log_Pj) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of the Naive Bayes Classifier with $log(1+f)$ frequency###\n",
    "**error rate on test set is 20.67%, slighlt higher than normal frequency count (i.e., $f$) method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TBD#\n",
    "vocab_DF=pd.read_csv('vocabulary.txt')\n",
    "####Removing stopwords####\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('english')\n",
    "####training on the 80% of the date with removing the stopwords####\n",
    "#TBD\n",
    "####measuring error with the 20% validation set with removing the stopwords####\n",
    "#TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
