{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#OAuth Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will try to scrape twitter data and do a tf-idf analysis on that (src-uwes twitter analysis). We will need OAuth authentication, and we will follow a similar approach as detailed in the yelp analysis notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import oauth2 as oauth\n",
    "import urllib2 as urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now need twitter api access. The following steps as available online will help you set up your twitter account and access the live 1% stream.\n",
    "\n",
    "1. Create a twitter account if you do not already have one.\n",
    "2. Go to https://dev.twitter.com/apps and log in with your twitter credentials.\n",
    "3. Click \"Create New App\"\n",
    "4. Fill out the form and agree to the terms. Put in a dummy website if you don't have one you want to use.\n",
    "5. On the next page, click the \"API Keys\" tab along the top, then scroll all the way down until you see the section \"Your Access Token\"\n",
    "6. Click the button \"Create My Access Token\". You can Read more about Oauth authorization online. \n",
    "\n",
    "Save the details of api_key, api_secret, access_token_key, access_token_secret in your vaule directory and load it in the notebook as shown in yelpSample notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Twitter_Keys\n",
    "TW_Keys=Twitter_Keys.getdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api_key = TW_Keys['api_key']#<get api key>\n",
    "api_secret = TW_Keys['api_secret']  #<get api secret>\n",
    "access_token_key = TW_Keys['access_token_key'] #<get your access token key here>\"\n",
    "access_token_secret = TW_Keys['access_token_secret'] #<get your access token secret here>\n",
    "\n",
    "\n",
    "_debug = 0\n",
    "\n",
    "oauth_token    = oauth.Token(key=access_token_key, secret=access_token_secret)\n",
    "oauth_consumer = oauth.Consumer(key=api_key, secret=api_secret)\n",
    "\n",
    "signature_method_hmac_sha1 = oauth.SignatureMethod_HMAC_SHA1()\n",
    "\n",
    "http_method = \"GET\"\n",
    "\n",
    "http_handler  = urllib.HTTPHandler(debuglevel=_debug)\n",
    "https_handler = urllib.HTTPSHandler(debuglevel=_debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a twitter request method which will use the above user logins to sign, and open a twitter stream request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTwitterStream(url, method, parameters):\n",
    "    req = oauth.Request.from_consumer_and_token(oauth_consumer,\n",
    "                                             token=oauth_token,\n",
    "                                             http_method=http_method,\n",
    "                                             http_url=url, \n",
    "                                             parameters=parameters)\n",
    "\n",
    "    req.sign_request(signature_method_hmac_sha1, oauth_consumer, oauth_token)\n",
    "\n",
    "    headers = req.to_header()\n",
    "\n",
    "    if http_method == \"POST\":\n",
    "        encoded_post_data = req.to_postdata()\n",
    "    else:\n",
    "        encoded_post_data = None\n",
    "        url = req.to_url()\n",
    "\n",
    "    opener = urllib.OpenerDirector()\n",
    "    opener.add_handler(http_handler)\n",
    "    opener.add_handler(https_handler)\n",
    "\n",
    "    response = opener.open(url, encoded_post_data)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above function to request a response as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we will test the above function for a sample data provided by twitter stream here -  \n",
    "url = \"https://stream.twitter.com/1/statuses/sample.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = []\n",
    "response = getTwitterStream(url, \"GET\", parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which will take a url and return the top 10 lines returned by the twitter stream\n",
    "\n",
    "** Note ** The response returned needs to be intelligently parsed to get the text data which correspond to actual tweets. This part can be done in a number of ways and you are encouraged to try different approaches to parse the response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"created_at\":\"Mon Dec 07 06:15:07 +0000 2015\",\"id\":673747547739975680,\"id_str\":\"673747547739975680\",\"text\":\"\\\\u3046\\\\u30fc\\\\u30fc\\\\u30fc\\\\u30fc\\\\u30fc\\\\u306f\\\\u3041\\\\u3042\\\\u3042\\\\u3042\\\\u3042\\\\u3042\\\\u3042\\\\u3042\\\\u3046\\\\u30fc\\\\u30fc\\\\u30fc\\\\u30fc\\\\u30fc\\\\u30fc\\\\u30fc\\\\u30fc\\\\u30fc\\\\u30fc\\\\u30fc\\\\u30fc\\\\u30fc\\\\u306f\\\\u3041\\\\u3042\\\\u3042\\\\u3042\\\\u3042\\\\u3042\\\\u3042\\\\u3042\\\\u3042\\\\u3042\\\\u3042\\\\uff01\\\\uff01\\\\uff01\",\"source\":\"\\\\u003ca href=\\\\\"http:\\\\/\\\\/twittbot.net\\\\/\\\\\" rel=\\\\\"nofollow\\\\\"\\\\u003etwittbot.net\\\\u003c\\\\/a\\\\u003e\",\"truncated\":false,\"in_reply_to_status_id\":null,\"in_reply_to_status_id_str\":null,\"in_reply_to_user_id\":null,\"in_reply_to_user_id_str\":null,\"in_reply_to_screen_name\":null,\"user\":{\"id\":2257643376,\"id_str\":\"2257643376\",\"name\":\"\\\\u30b4\\\\u30ea\\\\u62bc\\\\u3057\\\\u6295\\\\u3052\\\\u592a\\\\u90ce\",\"screen_name\":\"nagetarou_u_tan\",\"location\":null,\"url\":null,\"description\":\"\\\\u304a\\\\u3063\\\\u307a\\\\u3051\\\\u30e9\\\\u30fc\\\\u6cb9\\\\u30bb\\\\u30f3\\\\u30bf\\\\u30fc\\\\u6240\\\\u5c5e\\\\u3002\\\\u56db\\\\u5929\\\\u738b\\\\u3002\\\\u30aa\\\\u30e9\\\\u30f3\\\\u30a6\\\\u30fc\\\\u30bf\\\\u30f3\\\\u6700\\\\u5927\\\\u306e\\\\u525b\\\\u'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.read(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-66114949150f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(?<=,\"text\":).*(?=,\"source)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "re.search(r'(?<=,\"text\":).*(?=,\"source)', response.read(1000).decode('utf-8')).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####modified the input of fetchData to include the number of requested tweets####\n",
    "def fetchData(url,tweet_count=10):\n",
    "    parameters = []\n",
    "    top_tweet=[]\n",
    "    \n",
    "    if 'stream.twitter.com' in url:\n",
    "        response = getTwitterStream(url, \"GET\", parameters)\n",
    "        i=0\n",
    "        for line in response:\n",
    "            try:\n",
    "                if i<tweet_count:\n",
    "                    i+=1\n",
    "                    top_tweet.append(json.loads(line)['text'])\n",
    "                else:\n",
    "                    break\n",
    "            except:\n",
    "                i-=1\n",
    "                pass\n",
    "        return top_tweet\n",
    "    \n",
    "    elif 'api.twitter.com' in url:\n",
    "        url=url+\"&count=\"+str(tweet_count)\n",
    "        response = getTwitterStream(url, \"GET\", parameters)\n",
    "        response_content=json.loads(response.read())['statuses']\n",
    "        i=0\n",
    "        content=[]\n",
    "        for line in response_content:\n",
    "            try:\n",
    "                if i<tweet_count:\n",
    "                    i+=1\n",
    "                    top_tweet.append(line['text'])\n",
    "                else:\n",
    "                    break\n",
    "            except:\n",
    "                i-=1\n",
    "                pass\n",
    "        return top_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 streamed tweets are:\n",
      "\n",
      "\n",
      "tweet 1 is: ユニフォーム泥だらけになって\n",
      "腕も泥だらけになって\n",
      "そんながんばってる姿が\n",
      "とってもかっこいいと\n",
      "改めて思った1日だった \n",
      "\n",
      "tweet 2 is: ｸｳｳ \n",
      "\n",
      "tweet 3 is: bwisit talaga kainis kapal talaga \n",
      "\n",
      "tweet 4 is: @SC_YBora90 siang ya \n",
      "\n",
      "tweet 5 is: @azuuuuuuuchan まさかの鴻巣だったフラワーバス \n",
      "\n",
      "tweet 6 is: RT @justink154: @DailySexSupply @SexualGif LOL \n",
      "\n",
      "tweet 7 is: RT @WORIDSTARHIPHOP: LMAO https://t.co/jgUetRqdJz \n",
      "\n",
      "tweet 8 is: RT @BieberBonerz: this video saved me OMG 😍😍 https://t.co/MQf85wPyBT \n",
      "\n",
      "tweet 9 is: Ah no. Mañana a las ocho estoy durmiendo. Me dan estas noticias a estas horas. \n",
      "\n",
      "tweet 10 is: RT @MalditangMayora: Yung nag-aalok ka ng pagkain pero secretly hoping na sana ayaw nila. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://stream.twitter.com/1/statuses/sample.json\"\n",
    "top_10_tweet=fetchData(url)\n",
    "print \"Top 10 streamed tweets are:\\n\\n\"\n",
    "i=1\n",
    "for tweet in top_10_tweet:\n",
    "    print \"tweet %d is:\"%(i),tweet,\"\\n\"\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can also request twitter stream data for specific search parameters as follows\n",
    "url= \"https://api.twitter.com/1.1/search/tweets.json?q=\"+search_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the fetchData function to fetch latest live stream data for following search queries and output the first 5 lines\n",
    "\n",
    "1. \"UCSD\"\n",
    "2. \"Donald Trump\"\n",
    "3. \"Syria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 tweets about UCSD is:\n",
      "\n",
      "\n",
      "ユニフォーム泥だらけになって\n",
      "腕も泥だらけになって\n",
      "そんながんばってる姿が\n",
      "とってもかっこいいと\n",
      "改めて思った1日だった \n",
      "\n",
      "ｸｳｳ \n",
      "\n",
      "bwisit talaga kainis kapal talaga \n",
      "\n",
      "@SC_YBora90 siang ya \n",
      "\n",
      "@azuuuuuuuchan まさかの鴻巣だったフラワーバス \n",
      "\n",
      "RT @justink154: @DailySexSupply @SexualGif LOL \n",
      "\n",
      "RT @WORIDSTARHIPHOP: LMAO https://t.co/jgUetRqdJz \n",
      "\n",
      "RT @BieberBonerz: this video saved me OMG 😍😍 https://t.co/MQf85wPyBT \n",
      "\n",
      "Ah no. Mañana a las ocho estoy durmiendo. Me dan estas noticias a estas horas. \n",
      "\n",
      "RT @MalditangMayora: Yung nag-aalok ka ng pagkain pero secretly hoping na sana ayaw nila. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Top 5 tweets about Donald%20Trump is:\n",
      "\n",
      "\n",
      "ユニフォーム泥だらけになって\n",
      "腕も泥だらけになって\n",
      "そんながんばってる姿が\n",
      "とってもかっこいいと\n",
      "改めて思った1日だった \n",
      "\n",
      "ｸｳｳ \n",
      "\n",
      "bwisit talaga kainis kapal talaga \n",
      "\n",
      "@SC_YBora90 siang ya \n",
      "\n",
      "@azuuuuuuuchan まさかの鴻巣だったフラワーバス \n",
      "\n",
      "RT @justink154: @DailySexSupply @SexualGif LOL \n",
      "\n",
      "RT @WORIDSTARHIPHOP: LMAO https://t.co/jgUetRqdJz \n",
      "\n",
      "RT @BieberBonerz: this video saved me OMG 😍😍 https://t.co/MQf85wPyBT \n",
      "\n",
      "Ah no. Mañana a las ocho estoy durmiendo. Me dan estas noticias a estas horas. \n",
      "\n",
      "RT @MalditangMayora: Yung nag-aalok ka ng pagkain pero secretly hoping na sana ayaw nila. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Top 5 tweets about Syria is:\n",
      "\n",
      "\n",
      "ユニフォーム泥だらけになって\n",
      "腕も泥だらけになって\n",
      "そんながんばってる姿が\n",
      "とってもかっこいいと\n",
      "改めて思った1日だった \n",
      "\n",
      "ｸｳｳ \n",
      "\n",
      "bwisit talaga kainis kapal talaga \n",
      "\n",
      "@SC_YBora90 siang ya \n",
      "\n",
      "@azuuuuuuuchan まさかの鴻巣だったフラワーバス \n",
      "\n",
      "RT @justink154: @DailySexSupply @SexualGif LOL \n",
      "\n",
      "RT @WORIDSTARHIPHOP: LMAO https://t.co/jgUetRqdJz \n",
      "\n",
      "RT @BieberBonerz: this video saved me OMG 😍😍 https://t.co/MQf85wPyBT \n",
      "\n",
      "Ah no. Mañana a las ocho estoy durmiendo. Me dan estas noticias a estas horas. \n",
      "\n",
      "RT @MalditangMayora: Yung nag-aalok ka ng pagkain pero secretly hoping na sana ayaw nila. \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "queries=[\"UCSD\",\"Donald%20Trump\",\"Syria\"]\n",
    "for query in queries:\n",
    "    url= \"https://api.twitter.com/1.1/search/tweets.json?q=\"+query\n",
    "    top_10_tweet=fetchData(url)\n",
    "    top_5_tweet=fetchData(url, tweet_count=5)\n",
    "    print \"Top 5 tweets about %s is:\\n\\n\"%(query)\n",
    "    for tweet in top_5_tweet:\n",
    "        print tweet,'\\n'\n",
    "    print \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF###\n",
    "\n",
    "tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.It is among the most regularly used statistical tool for word cloud analysis. You can read more about it online (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "We base our analysis on the following\n",
    "\n",
    "1. The weight of a term that occurs in a document is simply proportional to the term frequency\n",
    "2. The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs\n",
    "\n",
    "For this question we will perform tf-idf analysis o the stream data we retrieve for a given search parameter. Perform the steps below\n",
    "\n",
    "1. use the twitterreq function to search for the query \"syria\" and save the top 200 lines in the file twitterStream.txt\n",
    "2. load the saved file and output the count of occurrences for each term. This will be your term frequency\n",
    "3. Calculate the inverse document frequency for each of the term in the output above.\n",
    "4. Divide the term frequency for each of the term by corresponding inverse document frequency.\n",
    "5. Sort the terms in the descending order based on their term freq/inverse document freq scores \n",
    "6. Print the top 10 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####Solving the problem using the fetchData function above and RESTful API####\n",
    "\n",
    "url= \"https://api.twitter.com/1.1/search/tweets.json?q=Syria\"\n",
    "top_200_tweet=fetchData(url, tweet_count=200)\n",
    "with open('twitterStream.txt','wb') as ts:\n",
    "    [ts.write(tweet.lower().encode('utf-8')+'\\n') for tweet in top_200_tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####Based on the clarification on piazza, I considered each tweet as one single documents and hence tf is the word\n",
    "#frequency within each document\n",
    "from collections import Counter\n",
    "tf=[]\n",
    "tweet_file=open('twitterStream.txt','rb')\n",
    "\n",
    "for line in tweet_file:\n",
    "    for char in '-.,\"\\n':\n",
    "        line=line.replace(char,' ')\n",
    "    c=Counter(line.split())\n",
    "    tf.append(c)\n",
    "\n",
    "tweet_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "idf_count={}\n",
    "idf={}\n",
    "for doc in range(len(tf)):\n",
    "    for term in set(tf[doc]):\n",
    "        idf_count[(doc,term)]=1\n",
    "        tweet_file=open('twitterStream.txt','rb')\n",
    "        for line in tweet_file:\n",
    "            if term in line:\n",
    "                idf_count[(doc,term)]+=1\n",
    "        tweet_file.close()\n",
    "        idf[(doc,term)]=np.log(len(tf)/idf_count[(doc,term)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf={}\n",
    "for doc in range(len(tf)):\n",
    "    for term in set(tf[doc]):\n",
    "        tf_idf[(term,tf[doc][term],idf[(doc,term)])]=tf[doc][term] * idf[(doc,term)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_tf_idf=sorted(tf_idf.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+---------------+----------+\n",
      "| tf |      idf      |  td-idf score |   term   |\n",
      "+----+---------------+---------------+----------+\n",
      "| 3  |  3.0169344812 | 9.05080344361 |  attack  |\n",
      "| 2  | 3.57655026914 | 7.15310053828 |  bombed  |\n",
      "| 2  | 3.35340671783 | 6.70681343565 |   like   |\n",
      "| 2  | 3.35340671783 | 6.70681343565 |   can    |\n",
      "| 2  |  3.0169344812 | 6.03386896241 | petition |\n",
      "| 2  |  3.0169344812 | 6.03386896241 |  signed  |\n",
      "| 2  | 2.88340308858 | 5.76680617716 |   ban    |\n",
      "| 2  | 2.88340308858 | 5.76680617716 |  #isis   |\n",
      "| 2  | 2.76562005292 | 5.53124010585 |   any    |\n",
      "| 2  | 2.47793798047 | 4.95587596094 |   and    |\n",
      "+----+---------------+---------------+----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "pt = PrettyTable(field_names=['tf', 'idf', 'td-idf score']) \n",
    "[pt.add_row((tf_idf_row[0][1],tf_idf_row[0][2],tf_idf_row[1])) for tf_idf_row in sorted_tf_idf ]\n",
    "pt.add_column('term',[tf_idf_row[0][0] for tf_idf_row in sorted_tf_idf])\n",
    "pt.padding_width = 1\n",
    "pt.align='c' # Set column alignment\n",
    "print pt[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####Solving the same problem using Tweepy library####\n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "import sys \n",
    "sys.path.append('/Users/phesami/Documents/DSE/phesami/DSE200/day_5_mining_the_Social_web/exercises/')\n",
    "import Twitter_Keys\n",
    "#consumer key, consumer secret, access token, access secret.\n",
    "atoken,csecret,ckey, asecret=Twitter_Keys.getkeys()\n",
    "\n",
    "class listener(StreamListener):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(listener, self).__init__()\n",
    "        self.counter=1\n",
    "        self.tweet_count=200\n",
    "        self.ts=open('twitterStream_Tweepy.txt','wb')\n",
    "\n",
    "    def on_status(self, status):\n",
    "        while self.counter<self.tweet_count:\n",
    "            self.ts.write(status.text.lower().encode('utf-8')+'\\n')\n",
    "            self.counter+=1\n",
    "            return True\n",
    "        self.ts.close()\n",
    "        return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print \"error\",status\n",
    "\n",
    "\n",
    "auth = OAuthHandler(ckey, csecret)\n",
    "auth.set_access_token(atoken, asecret)\n",
    "\n",
    "twitterStream = Stream(auth, listener())\n",
    "twitterStream.filter(track=[\"syria\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+---------------+------------+\n",
      "| tf |      idf      |  td-idf score |    term    |\n",
      "+----+---------------+---------------+------------+\n",
      "| 2  | 5.04342511692 | 10.0868502338 |  #germany  |\n",
      "| 2  | 5.04342511692 | 10.0868502338 |   labour   |\n",
      "| 2  | 5.04342511692 | 10.0868502338 | supporting |\n",
      "| 2  | 4.35027793636 | 8.70055587272 | jerusalem  |\n",
      "| 2  | 4.35027793636 | 8.70055587272 |  deserves  |\n",
      "| 2  | 4.35027793636 | 8.70055587272 |    post    |\n",
      "| 2  | 4.35027793636 | 8.70055587272 |   called   |\n",
      "| 2  | 4.12713438505 | 8.25426877009 |   their    |\n",
      "| 2  | 3.79066214842 | 7.58132429685 |   &amp;    |\n",
      "| 2  | 3.25166564769 | 6.50333129538 |    you     |\n",
      "+----+---------------+---------------+------------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "from collections import Counter\n",
    "tf=[]\n",
    "tweet_file=open('twitterStream_Tweepy.txt','rb')\n",
    "\n",
    "for line in tweet_file:\n",
    "    c=Counter(line.split())\n",
    "    tf.append(c)\n",
    "\n",
    "tweet_file.close()\n",
    "\n",
    "idf_count={}\n",
    "idf={}\n",
    "for doc in range(len(tf)):\n",
    "    for term in set(tf[doc]):\n",
    "        idf_count[(doc,term)]=1\n",
    "        tweet_file=open('twitterStream_Tweepy.txt','rb')\n",
    "        for line in tweet_file:\n",
    "            if term in line:\n",
    "                idf_count[(doc,term)]+=1\n",
    "        tweet_file.close()\n",
    "        idf[(doc,term)]=np.log(len(tf)/idf_count[(doc,term)])\n",
    "        \n",
    "        \n",
    "tf_idf={}\n",
    "for doc in range(len(tf)):\n",
    "    for term in set(tf[doc]):\n",
    "        tf_idf[(term,tf[doc][term],idf[(doc,term)])]=tf[doc][term] * idf[(doc,term)]\n",
    "        \n",
    "import operator\n",
    "sorted_tf_idf=sorted(tf_idf.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "pt = PrettyTable(field_names=['tf', 'idf', 'td-idf score']) \n",
    "[pt.add_row((tf_idf_row[0][1],tf_idf_row[0][2],tf_idf_row[1])) for tf_idf_row in sorted_tf_idf ]\n",
    "pt.add_column('term',[tf_idf_row[0][0] for tf_idf_row in sorted_tf_idf])\n",
    "pt.padding_width = 1\n",
    "pt.align='c' # Set column alignment\n",
    "print pt[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
